{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 2973,
     "status": "ok",
     "timestamp": 1607721366876,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "8_RV4TjiX9ek",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38e33a35089d877704c3c9031e2bb0a7",
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', 100) # set to larger value to see all the columns of pd.DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gRbVNiv_X9el",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c756717bce4ffc4bbdfba12601ab6bd3",
     "grade": false,
     "grade_id": "toc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "This is programming assignment for week 3. In this assignment you will be practicing with feature selection and hyperparameters tuning. \n",
    "\n",
    "Please, read all the notebook carefully and make sure that you understand not only the task, but the whole pipeline.\n",
    "\n",
    "### Grading\n",
    "The assignemnt is automatically graded. \n",
    "\n",
    "**Automatic grading**\n",
    "After you finish solving all the tasks restart the kernel (`kernel -> restart`) and click the button `Validate` to check that everything works as expected. Afterwards, you can submit your work.\n",
    "\n",
    "\n",
    "# Table of Contents:\n",
    "* [Problem 1.](#part1)  Feature selection\n",
    "     - [Task 1](#task1) [1 pt]\n",
    "     - [Task 2](#task2) [1 pts]\n",
    "     - [Task 3](#task3) [2 pts]\n",
    "     - [Task 4](#task4) [1 pt]\n",
    "     - [Task 5](#task5) [1 pts]\n",
    "     - [Task 6](#task6) [2 pts]\n",
    "     - [Task 7](#task7) [1 pts]\n",
    "   \n",
    "* [Problem 2](#part2). Hyperparameters Tuning\n",
    "    - [Task 1](#task2_1) [1 pts]\n",
    "    - [Task 2](#task2_2) [1 pts]\n",
    "\n",
    "\n",
    "\n",
    "## Problem 1. Feature selection  <a class=\"anchor\" id=\"part1\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2BPpghrgR7U"
   },
   "source": [
    "We will work with a Mercedes-Benz Greener Manufacturing dataset and try different feature engineering, feature selection and dimensionality reduction techniques to predict the time Mercedes-Benz cars spend on the test bench.\n",
    "\n",
    "First, let's load the data and take a look at the dataset. Target variable to predict is y, the time for testing the car in seconds. We will exclude it from the feature set, as well as a column with unique car id, and split the dataset into training and validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1753,
     "status": "ok",
     "timestamp": 1607721368558,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "AjtuTrQCX9el",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3403a045fe5c35516e3b237b74e52eff",
     "grade": false,
     "grade_id": "load_data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ef11bd36-9fff-427e-a14e-f29b8b8dcb45"
   },
   "outputs": [],
   "source": [
    "file_path = \"https://github.com/mbburova/MDS/raw/main/train_mercedes.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1620,
     "status": "ok",
     "timestamp": 1607721368559,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "nIOM1uRFX9em",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b18c0ebe119883f9b557cb1abecf040a",
     "grade": false,
     "grade_id": "cell-382170cdb4635f79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df[\"y\"]\n",
    "X = df.drop([\"ID\", \"y\"], axis=1)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a6FJ-WcZX9em",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7aca6a1d7770857d5c8951dfcaa4445",
     "grade": false,
     "grade_id": "cell-f46c58f6ac0cf391",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ajnwJ46yIvQu",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "922cacf512c0cf03b5114b2e69a5a610",
     "grade": false,
     "grade_id": "baseline",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Baseline model\n",
    "\n",
    "As we can see from a simple exploratory analysis, this dataset contains only categorical and ordinal features. Also, there are no missing values in any features. \n",
    "\n",
    "Thus, as a very first baseline model, we can just one-hot encode categorical variables and apply linear regression with $l_2$-regularization to predict target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1607721372598,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "vMgmeatqItf2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "342860786c609020f72867377e1a7f95",
     "grade": false,
     "grade_id": "cell-b8eb53b7de13c2c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a89aa0a8-d865-49c8-8ef7-a4e562228933"
   },
   "outputs": [],
   "source": [
    "bin_cols = X_tr.columns[X_tr.dtypes == \"int64\"].tolist() \n",
    "print(\"Number of binary features =\", len(bin_cols))\n",
    "cat_cols = X_tr.columns[X_tr.dtypes == \"object\"].tolist()\n",
    "print(\"Number of categorical features =\", len(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1607721372904,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "Lk2URmX4KHEM",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ff9e1f312ccae5d11a73233c23d7984",
     "grade": false,
     "grade_id": "cell-e054aa1d38ab5f7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d6c6d665-d40f-4e5c-a89d-442b0c434e64"
   },
   "outputs": [],
   "source": [
    "print(\"Number of columns with any missing values:\")\n",
    "print(X_tr.isnull().any(axis=0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FlVc8ZgdxxC2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21949b863042619c2ebd64c252b798ca",
     "grade": false,
     "grade_id": "cell-b0e27e9766b1be40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.1** [1 pt] <a class=\"anchor\" id=\"task1\"></a>\n",
    "\n",
    "We will evaluate model prediciton quality with $R^2$-score metric. Implement a function to evaluate performance of a given model both on the train and test sets. The function should\n",
    "- take as input a model (possibly a pipeline), as well as feature matrix and target variable for both train and test sets,\n",
    "- fit the model on the train set,\n",
    "- compute model predictions for train and test sets,\n",
    "- assess the quality of both predictions with $R^2$ score,\n",
    "- return train and test $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1607721381787,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "72CxGkHoLCwV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7532bb0a134f0f485ea494522839420",
     "grade": false,
     "grade_id": "task1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def evaluate_model(model, X_tr, y_tr, X_te, y_te):\n",
    "    # set a fixed random seed for model weights initialization \n",
    "    # to obtain reproducible results.\n",
    "    # Do not change the value.\n",
    "    if isinstance(model, Pipeline):\n",
    "        model.set_params(estimator__random_state=0);    \n",
    "    else:\n",
    "        model.set_params(random_state=0);    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return R2_tr, R2_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1607722076758,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "PLcVoilpqdnJ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1112808534daeb34640c6eff84b634e6",
     "grade": true,
     "grade_id": "cell-task1_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# TEST evaluate_model \n",
    "res = evaluate_model(Ridge(), X_tr.iloc[:100, 10:11], y_tr[:100], X_tr.iloc[100:150, 10:11], y_tr[100:150])\n",
    "print('R2 on train:', res[0])\n",
    "print('R2 on test:', res[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FEt2ZvtshUDl",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f839e662f2cc3bd387a9f39523e02ef4",
     "grade": false,
     "grade_id": "cell-db6ab8eacc9edcff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.2** [1 pt] <a class=\"anchor\" id=\"task2\"></a>\n",
    "\n",
    "Initialize the baseline model. It should represent a pipeline including ohe-hot encoding of categorical variables and a `Ridge` regression (`sklearn` imlementation of linear regression with $l_2$ regularization) model with default parameters. \n",
    "\n",
    "- create `col_transformer` - ColumnTransformer pipeline with a single transformer - `OneHotEncoder` applied to categorical columns (**cat_cols**) only. To keep other (binary) features, use `remainder=passthrough` argument of `ColumnTransformer`.\n",
    "- create `model` - Pipeline with two steps:\n",
    "    - **col_transformer** you created before (used to encode categorical features)\n",
    "    - **estimator** should be a `Ridge` regression with default parameters. *Please, make sure that this step is named exactly **estimator**. It will be used in the latter tasks.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 644,
     "status": "ok",
     "timestamp": 1607721801263,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "Lvm4m9BVLCwW",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c48d41ba72b411861146bbe9abb7117f",
     "grade": false,
     "grade_id": "cell-f8f2dd66042265d0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1607721935153,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "Cvj0XjXFk3OQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbd1343abcab12b8847c19bae2e43f4a",
     "grade": true,
     "grade_id": "cell-0822a1056ab386e0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST baseline model\n",
    "print('Column transformers:')\n",
    "print(col_transformer.transformers)\n",
    "\n",
    "print('Pipeline:')\n",
    "print(model.steps[0])\n",
    "print(model.steps[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Pi2i654rkggV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a52745488d76b0f910d9c90691ab69f",
     "grade": false,
     "grade_id": "cell-b9226690cbee12ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can evaluate the baseline model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1607721936742,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "rLGaH_1bkgBm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51931e59368a66e0e72fa83022c772d1",
     "grade": false,
     "grade_id": "cell-bd6d504241fec606",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "145bd0b2-9981-45d1-f01b-69c25b0beaed"
   },
   "outputs": [],
   "source": [
    "R2_tr, R2_te = evaluate_model(model, X_tr, y_tr, X_te, y_te)\n",
    "print(\"Train R2 = %.2f\" % R2_tr)\n",
    "print(\"Test R2 = %.2f\" % R2_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8oKwF-86BDb"
   },
   "source": [
    "As we can see, model performance on the training set is slightly higher than on the test data. This can mean that there is overfitting, which is especially likely when number of features is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 971,
     "status": "ok",
     "timestamp": 1607696670490,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "-icV7Z3VLYJG",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce6a16547a7d4c9c43f15ad527e9be4b",
     "grade": false,
     "grade_id": "cell-765bc409fa568008",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e25af6ba-944f-4f7d-afba-f5bae6949557"
   },
   "outputs": [],
   "source": [
    "print(\"Number of features before one-hot-encoding: \")\n",
    "print(X_tr.shape[1])\n",
    "print()\n",
    "print(\"Number of features after one-hot-encoding: \")\n",
    "print(col_transformer.fit_transform(X_tr).shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmFHxU_23l0w"
   },
   "source": [
    "\n",
    "As we can see, there were quite a lot of features in the original sample, and after the one-hot encoding of categorical variables, their number increased by almost one and a half times.\n",
    "\n",
    "\n",
    "To avoid overfitting, it can be helpful to reduce the number of variables by selecting the most important ones. In our sample, all the variables are anonymized. However we can still select the most relevant based not on their physical meaning, but on their observed predictive power and informativeness for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rs_iw64OMKXs",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f458122bafda4a67fc1029de76bfd38a",
     "grade": false,
     "grade_id": "cell-105edfbad47a5ceb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Filter-based methods\n",
    "<!-- ### Univariate feature selection (filter-based methods) -->\n",
    "\n",
    "The most simple group of feature selection approaches are filter-based methods or univariate feature selection. These methods are based on analysis of individual predictive power of each variable. In our case, since we have a regression problem, we can simply check the correlation of each feature with the target variable. \n",
    "\n",
    "In `sklearn`, you can find a ready-to-use implementation of univariate feature selection algorithm - `SelectKBest` transformer, which filters out a set of **k** features basen on the values of **scoring function**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3p7nq4q8x2f"
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.3** [2 pts] <a class=\"anchor\" id=\"task3\"></a>\n",
    "\n",
    "Implement a function to compute absolute value of correlation of each feature variable with the target variable.\n",
    " The function should:\n",
    "- take as input the feature matrix and the target variable,\n",
    "- compute correlation coefficient of each feature in the feature matrix and the target variable,\n",
    "- return a vector of correlation coefficient for all features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1607722001251,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "DjeTNCs8OneF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8c986c739de50bd70527771528f592f",
     "grade": false,
     "grade_id": "cell-e3e09dda53672211",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_abs_corr_coef(X, y):\n",
    "    \"\"\"\n",
    "    Compute\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray of shape (n_samples, n_features)\n",
    "        Feature matrix.\n",
    "    y : numpy.ndarray of shape (n_samples,)\n",
    "        Target variable\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    corr_coefs : numpy.ndarray of shape (n_features,)\n",
    "        Vector of absolute values of correlation coefficients \n",
    "        for all features\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return corr_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1607722001498,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "Y4BSisDsVR96",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb644216c4954578ae24b3dcee6dfd5e",
     "grade": true,
     "grade_id": "cell-8a3ad3d3ffaf0ab6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST get_abs_corr_coef\n",
    "A = np.array(\n",
    "    [[1, 0, 0],\n",
    "     [0, 0.5, -0.5],\n",
    "     [0, 1, -1]]\n",
    ")\n",
    "b = [0, 1, 1]\n",
    "print(get_abs_corr_coef(A, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSEpixMc-wFM"
   },
   "source": [
    "Now we can check how model performance changes if we use only `k=60` variables with strongest correlation (both positive and negative) with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-uNETMVlcpT"
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.4** [1 pt] <a class=\"anchor\" id=\"task4\"></a>\n",
    "\n",
    "Initialize the model with filter-based feature selection. \n",
    "\n",
    "\n",
    "Create `model_k_best` - Pipeline with three steps:\n",
    "- **col_transformer** that you've created in task 1.2\n",
    "- **feat_selector**  - `SelectKBest` transformer. It should use `get_abs_corr_coef` scoring function to select `30` features. \n",
    "- **estimator** - `Ridge` regression with default parameters. *Please, make sure that this step is named exactly **estimator**. It will be used in the latter tasks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1607722245073,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "A-Hjogvtlcpz",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65f2dc1f37941d122f6af66903144c64",
     "grade": false,
     "grade_id": "cell-88ffd9297ff3d54b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "feat_selector = SelectKBest(get_abs_corr_coef, k=20)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1607722246332,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "4QF7OVgelcp1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8afab9e52cbf0f231cb92d0fcf40e514",
     "grade": true,
     "grade_id": "cell-e584763198adafe0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST model with filter-based feature selection\n",
    "\n",
    "\n",
    "print('Pipeline:')\n",
    "print(model_k_best.steps[0])\n",
    "print(model_k_best.steps[1])\n",
    "print(model_k_best.steps[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1607722250726,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "FPYmRU8KW-39",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47d48aca4247896f7a16ffc37965f940",
     "grade": false,
     "grade_id": "cell-70b8d4de35cc602f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "260a03c4-6e83-417d-a626-9e8753f80aa6"
   },
   "outputs": [],
   "source": [
    "R2_tr_kbest, R2_te_kbest = evaluate_model(model_k_best, X_tr, y_tr, X_te, y_te)\n",
    "print(\"Train R2 = %.2f\" % R2_tr_kbest)\n",
    "print(\"Test R2 = %.2f\" % R2_te_kbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vVABUorUOryi",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "619c1fef8677f8288cfedbb2c5b22954",
     "grade": false,
     "grade_id": "wrapper",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Wrapper methods \n",
    "\n",
    "In wrapper methods, feature selection process is based on the greedy search.  Different combinations of features are evaluated and compared using the evaluation criterion, which is simply the performance of the trained model, measured with any appropriated score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dDtCK21uXBUv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ada3946154e60998fc34e102cd243cfb",
     "grade": false,
     "grade_id": "cell-094c6952c4f450fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sklearn has a ready-to-use implementation of a wrapper feature selection algorithm called **Recursive Feature Elimination** (RFE). \n",
    "\n",
    "The algorithm works by iteratively eliminating features which turned out to be least important based on their weights (or other importance scores) in the trained model. For instance, if the linear regression is used as a base algorithm in selection process, the model is first trained on the whole feature set. Then, the features with smallest weights (coefficients) are excluded, and the model is re-trained on the remaining feature set. The process is repeated until the desired number of features is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qIZGHZANm9ks",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fca298a7745b60321ad831416b8b857",
     "grade": false,
     "grade_id": "cell-dc9fe5b1b4616c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.5** [1 pt] <a class=\"anchor\" id=\"task5\"></a>\n",
    "\n",
    "Initialize the model with Recursive Feature Elimination method for feature selection.\n",
    "\n",
    "\n",
    "\n",
    "Create `model_rfe` - Pipeline with three steps:\n",
    "- **col_transformer** that you've created in task 1.2\n",
    "- **feat_selector**  - `RFE` transformer. It should use `Ridge` regression with default parameters as an internal estimator to select `20` features. \n",
    "- **estimator** - `Ridge` regression with default parameters. \n",
    "\n",
    "\n",
    "*Note, that instance of Ridge regression used in `RFE` transformer should not be the same used as estimator. You need 2 independent instances of th esame model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1607723973492,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "mtzFI_DMOngD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb737e1a458117a756d88af921c437ae",
     "grade": false,
     "grade_id": "cell-868f855ab31ed1c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1607723987626,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "-7hz0QGkt1Yn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb076aa1962e29deffe667b2fb6eb7eb",
     "grade": true,
     "grade_id": "cell-6ebcf0a3441cdf34",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST model with wrapper feature selection\n",
    "\n",
    "\n",
    "print('Pipeline:')\n",
    "print(model_rfe.steps[0])\n",
    "print(model_rfe.steps[1])\n",
    "print(model_rfe.steps[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1d844e628dcdd88f1a34c87d1df35a2",
     "grade": false,
     "grade_id": "cell-b083a4e98508c22b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Evaluate the model (it may take some time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 17918,
     "status": "ok",
     "timestamp": 1607724008925,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "2F5bo1o2tyJS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e2224d306b64e2188da988c24333e2b",
     "grade": false,
     "grade_id": "cell-c7c6e57c5ebdab90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "03071ff9-b7c2-4cd2-c776-2cf13d0e2763"
   },
   "outputs": [],
   "source": [
    "R2_tr_rfe, R2_te_rfe = evaluate_model(model_rfe, X_tr, y_tr, X_te, y_te)\n",
    "print(\"Train R2 = %.2f\" % R2_tr_rfe)\n",
    "print(\"Test R2 = %.2f\" % R2_te_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaKQnoatZ8Hx"
   },
   "source": [
    "Two widely used wrapper feature selection approaches a **Forward** and **Backward** sequential feature selection. \n",
    "\n",
    "In **Forward feature selection** method, one starts from an empty set of selected features. Then, in each iteration, the feature that improves the model performance most is added to the feature set. The process continues until desired number of features is achieved or an addition of a new feature does not improve the model performance.\n",
    "\n",
    "In **Backward feature selection** method, one starts from the set of all features and iteratively removes the feature, the elimination of which improves the model performance most. Again, the process continues until desired number of features is achieved or an elimination of another feature does not improve the model performance.\n",
    "\n",
    "In the cases when number of feature is very large, and number of informative ones is expected to be considerably smaller, backward feature selection algorithm can take very long time to converge, and forward feature selection is usually preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTvZwVhEKd9d"
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.6** [2 pts] <a class=\"anchor\" id=\"task6\"></a>\n",
    "\n",
    "Implement a transformer class to perform forward feature selection of a predefined number of features. \n",
    "- Start from an **empty** set of **selected features**. \n",
    "- At each step, train the model on the current set of selected features and one of the yet not selected features - **candidate features**. Score model either on the training, or on validation set and collect model scores for all candidate features.\n",
    "- Select from candidate features the feature, that improved model performance most, add it to the set of selected features and eliminate from candidate features.\n",
    "- Repeat until the desired number of selected features is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 672,
     "status": "ok",
     "timestamp": 1607724272651,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "sSNHOZOLOnh8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb8b5896b5d15db64cd0451cb0ba9bd1",
     "grade": false,
     "grade_id": "cell-412fe35573be7e79",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class ForwardFeatureSelection(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Selects features by iteratively adding the features \n",
    "    which provides the highest model score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object\n",
    "        A supervised learning estimator with ``fit`` and ``score`` methods.\n",
    "    n_features_to_select : int \n",
    "        The number of features to select.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 estimator,\n",
    "                 n_features_to_select,\n",
    "                ):\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.n_features_to_select = n_features_to_select\n",
    "#         self.score_on_set = score_on_set\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform forward feature selection and save selected feature subset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame or numpy.ndarray of shape (n_samples, n_features)\n",
    "            Feature matrix.\n",
    "        y : pd.Series or numpy.ndarray of shape (n_samples,) (default : None)\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        if type(X) is not np.ndarray:\n",
    "            X = X.values\n",
    "        if type(y) is not np.ndarray:\n",
    "            y = y.values\n",
    "\n",
    "        feats_idx = list(range(X.shape[1])) # not yet selected features\n",
    "        self.selected_feats_idx = [] # selected features\n",
    "        self.selected_feats_scores = [] # scores of the selected features\n",
    "\n",
    "        for it in range(self.n_features_to_select):\n",
    "            scores = []\n",
    "            for i in feats_idx: \n",
    "                # use `self.estimator` to compute score after addition of the feature number `i`\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "            # select feature with thte best score\n",
    "            best_score_idx = np.argmax(scores)\n",
    "            self.selected_feats_scores.append(scores[best_score_idx])\n",
    "            self.selected_feats_idx.append(feats_idx[best_score_idx])\n",
    "            feats_idx.remove(feats_idx[best_score_idx])\n",
    "       \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Select features according to precomputed feature subset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (n_samples, n_features)\n",
    "            Feature matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : array-like of shape (n_samples, n_features)\n",
    "            Transformed feature matrix. \n",
    "        \"\"\"\n",
    "        if type(X) is not np.ndarray:\n",
    "            X = X.values\n",
    "        # use `self.selected_feats_idx` to select \"the best\" features\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xuORZ04REGrK",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b42a3b7492353eb37c17c9b43d5cb60",
     "grade": true,
     "grade_id": "cell-1c220d297c556a99",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST ForwardFeatureSelection on toy example\n",
    "test_data_X = X_tr.iloc[:100, 10:]\n",
    "test_data_y = y_tr[:100]\n",
    "\n",
    "selector = ForwardFeatureSelection(Ridge(), 2)\n",
    "\n",
    "# fit\n",
    "selector.fit(test_data_X, test_data_y)\n",
    "print('Selected columns:')\n",
    "print(test_data_X.columns[selector.selected_feats_idx])\n",
    "\n",
    "# transform\n",
    "X_transformed = selector.transform(test_data_X)\n",
    "print(X_transformed[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "R6slrrhCzlTh",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "095f38ef6c917f13107043b180305df2",
     "grade": false,
     "grade_id": "cell-c6982dade32e97c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.7** [1 pt] <a class=\"anchor\" id=\"task7\"></a>\n",
    "\n",
    "Initialize the model with Forward Feature Selection method. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Create `model_forward` - Pipeline with three steps:\n",
    "- **col_transformer** that you've created in task 1.2\n",
    "- **feat_selector**  - `ForwardFeatureSelection` transformer implemented in the previous task. It should use `Ridge` regression with default parameters as an internal estimator to select `20` features. \n",
    "- **estimator** - `Ridge` regression with default parameters.\n",
    "\n",
    "\n",
    "*Note, that instance of Ridge regression used in `ForwardFeatureSelection` transformer should not be the same used as estimator. You need 2 independent instances of th esame model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 654,
     "status": "ok",
     "timestamp": 1607724641586,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "tiG3lFKb2vhB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c70e7dc489568678e1ba95c7d097647f",
     "grade": false,
     "grade_id": "cell-5c1a5955756166d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NjPSz7HH2vhB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72c2f64f996ac4563ebd566a5b9e43ac",
     "grade": true,
     "grade_id": "cell-8687020d4055d994",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST model with ForwardFeatureSelection\n",
    "\n",
    "\n",
    "print('Pipeline:')\n",
    "print(model_forward.steps[0])\n",
    "print(model_forward.steps[1])\n",
    "print(model_forward.steps[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5fM9pls72vhC",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a2295d652759ff594ab7e580bedf93f",
     "grade": false,
     "grade_id": "cell-56f0728e7fe190a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Evaluate the model (it may take some time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36538,
     "status": "ok",
     "timestamp": 1607724680751,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "TNI-DSHK2vhC",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77ecceee56401987ca3cb59fe4ee4960",
     "grade": false,
     "grade_id": "cell-e350f2897ae1408e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c8b6b1ab-81c0-4b27-aa95-1eaea1984016"
   },
   "outputs": [],
   "source": [
    "R2_tr_forward, R2_te_forward = evaluate_model(model_forward, X_tr, y_tr, X_te, y_te)\n",
    "print(\"Train R2 = %.2f\" % R2_tr_forward)\n",
    "print(\"Test R2 = %.2f\" % R2_te_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b297072d2b8d8e6ba18c1524383d1d67",
     "grade": false,
     "grade_id": "cell-96a75ca45f7322fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us see, how the performance of the model was changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [R2_tr, R2_tr_kbest, R2_tr_rfe, R2_tr_forward]\n",
    "test_scores = [R2_te ,R2_te_kbest, R2_te_rfe, R2_te_forward]\n",
    "models = ['All features', 'KBest', 'RFE', 'Forward Selection']\n",
    " \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_scores)\n",
    "plt.plot(test_scores)\n",
    "plt.scatter(range(len(models)), train_scores, label=\"train\")\n",
    "plt.scatter(range(len(models)), test_scores, label=\"test\")\n",
    "plt.xticks(range(len(models)), models, rotation=30)\n",
    "plt.ylabel(\"Model's R2 score\", fontdict={\"size\" : 12})\n",
    "plt.title(\"Dependence of model's score on the features selection method\", fontdict={\"size\" : 14})\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUpJ43yOX9en"
   },
   "source": [
    "---\n",
    "## Problem 2. Model hyperparameter tuning  <a class=\"anchor\" id=\"part2\"></a>\n",
    "---\n",
    "\n",
    "In the previous task, we used Ridge regression with $l_2$ regularization to predict the target variable. The model hyperparameter `alpha` determines the strength of the regularization, but so far we have used its default value ($alpha = 1$), which may be not optimal. \n",
    "\n",
    "In addition, different models may require different degrees of regularization, and it would be incorrect to compare their quality with one fixed value. Now we will optimize the degree of regularization of our regression model.\n",
    "\n",
    "<!-- Among `sklearn.linear_models`, you can find implementation of the regression models with $l_1$ regularization (`Lasso`), $l_2$ regularization (`Ridge`), as well as their combination (`ElasticNet`). -->\n",
    "\n",
    "**Grid search** is commonly used to find the optimal value of the regularization coefficient (and other model hyperparameters). For each combination of hyperparameters, a model is trained, and its performance is measured on a validation set or using cross-validation. The set of hyperparameters at which the model achieves the best validation performance is optimal.\n",
    "\n",
    "Note that the test set on which the final model evaluation is performed should not be used as a validation set for hyperparameter tuning. Otherwise, overfitting occurs and the model quality measured on this test set no longer reflects its predictive ability for the new unseen data. \n",
    "\n",
    "`Sklearn` has ready-to-use implementation of grid search - `GridSearchCV` estimator.  It evaluates model performance on cross-validation for each hyperparameter set and selects the best model accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL9EO95DX9en"
   },
   "source": [
    "---\n",
    "**Task 2.1** [1 pt] <a class=\"anchor\" id=\"task2_1\"></a>\n",
    "\n",
    "\n",
    "Use `GridSearchCV` to find optimal $l_2$ regularization coefficient for two models: \n",
    "1. Baseline pipeline without feature selection (`model` from task 1.2)\n",
    "2. Pipeline with univariate feature selection (`model_k_best` from task 1.4).\n",
    "\n",
    "Then, compare which model is better with optimal hyperparameters.\n",
    "\n",
    "- We will use `alphas` - list of regularization coefficient (`estimator__alpha`) to search through. \n",
    "\n",
    "- Create two `GridSearchCV` objects \n",
    "    - `grid_cv` for baseline model \n",
    "    - `grid_cv_k_best` for model with feature selection. \n",
    "\n",
    "- For each model, fit `GridSearchCV` on the whole training set `X_tr`. Optimize $R^2$-score on 3-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 6337,
     "status": "ok",
     "timestamp": 1607725003256,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "J9YwXoNOX9en",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cd693b4367514e4f51bbfefb786aa8b",
     "grade": false,
     "grade_id": "cell-a2c6dc9a02792447",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "alphas = np.logspace(-4, 4, 9)\n",
    "param_grid = {\n",
    "    \"estimator__alpha\" : alphas,\n",
    "}\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1607725003261,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "DLAhWYYCX9en",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a304c603ee9cd56d3eea096253af473d",
     "grade": true,
     "grade_id": "cell-6614804c811b77b5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST grid_cv model\n",
    "assert \"estimator__alpha\" in grid_cv.param_grid\n",
    "assert \"estimator__alpha\" in grid_cv_k_best.param_grid\n",
    "assert grid_cv.scoring == 'r2'\n",
    "assert grid_cv_k_best.scoring == 'r2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPS2mq8S32bV"
   },
   "source": [
    "\n",
    "- Plot the dependence of the models' validation performances on the alpha value and visually compare how the quality of each model depends on the regularization degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1607725015194,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "SbNZdwOeX9en",
    "outputId": "77d8fc41-5cf8-4169-fc04-c387dd430fcf"
   },
   "outputs": [],
   "source": [
    "scores = grid_cv.cv_results_['mean_test_score']\n",
    "plt.plot(alphas, scores, label=\"without feature selection\")\n",
    "scores_k_best = grid_cv_k_best.cv_results_['mean_test_score']\n",
    "plt.plot(alphas, scores_k_best, label=\"with feature selection\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"CV score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Without feature selection:\")\n",
    "print(\"Optimal alpha  = %.4f\" % grid_cv.best_params_['estimator__alpha'])\n",
    "print(\"Optimal R2 score = %.4f\" % grid_cv.best_score_)\n",
    "print()\n",
    "print(\"With feature selection:\")\n",
    "print(\"Optimal alpha  = %.4f\" % grid_cv_k_best.best_params_['estimator__alpha'])\n",
    "print(\"Optimal R2 score = %.4f\" % grid_cv_k_best.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjpO1JW039Gz"
   },
   "source": [
    "- Select the better model, fix the optimal value of the regularization coefficient, fit it on the training set, and estimate its performance on the test set `X_te`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1607725056089,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "CZsVnlKeX9en",
    "outputId": "c9915fa5-2a6e-4314-9091-598f611f91fe"
   },
   "outputs": [],
   "source": [
    "model = grid_cv_k_best.best_estimator_\n",
    "R2_tr, R2_te = evaluate_model(model, X_tr, y_tr, X_te, y_te)\n",
    "\n",
    "print(\"Train R2 = %.2f\" % R2_tr)\n",
    "print(\"Test R2 = %.2f\" % R2_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwrTKS-WX9en"
   },
   "source": [
    "Also, we can perform the search of optimal number of selected features `k`. However, the optimal strength of regularization will also depend on the number of selected features, since models with larger number of features usually need stronger regularization. Thus, it is a good idea to perform grid search for two these parameters simultaneously.\n",
    "\n",
    "<!-- Also, we can still assume that some of the features generated by the one-hot encoding of categorical variables are redundant. In this case, you may want to use a $l_1$ -regularization, which is more likely to assign exactly zero weights to insignificant variables. Thus, an internal feature selection by importance is performed, and the final model depends on a smaller number of variables.\n",
    "\n",
    "Among `sklearn.linear_models`, you can find implementation of the regression models with $l_1$ regularization (`Lasso`), $l_2$ regularization (`Ridge`), as well as their combination (`ElasticNet`). -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7OFQaRnX9en"
   },
   "source": [
    "---\n",
    "**Task 2.2** [1pt] <a class=\"anchor\" id=\"task2_2\"></a>\n",
    "\n",
    "Perform grid search for 2 hyperparameters to find both optimal **regularization coefficient** value (`alpha`) and **number of features** ($k$) for the pipeline with univariate feature selection (`model_k_best` from task 1.4).\n",
    "\n",
    "Blow we define:\n",
    "- `alphas` - list of regularization coefficient to search through. \n",
    "- `ks` - list of number of features values to search through.\n",
    "\n",
    "Your task:\n",
    "- Create `grid_cv_k_best` \n",
    "    - `GridSearchCV` object\n",
    "    - Use `alphas` and `ks` to define parameter grid for estimator and feature selector\n",
    "\n",
    "- Fit `grid_cv_k_best` on the whole training set `X_tr`. Optimize $R^2$-score on 3-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 23330,
     "status": "ok",
     "timestamp": 1607726115016,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "4wivQoPHX9en",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "559c3a721f9ea8f352cfd547974acb23",
     "grade": false,
     "grade_id": "cell-dc307258540ac640",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_k_best.set_params(estimator__random_state=0);\n",
    "alphas = np.logspace(-4, 4, 9)\n",
    "ks = np.arange(20, 310, 30)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08fc12966bc84aaa631baa4541d11544",
     "grade": true,
     "grade_id": "cell-43bdb013d0e9bb1c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Best pipeline:')\n",
    "print(grid_cv_k_best.best_estimator_.steps[0])\n",
    "print(grid_cv_k_best.best_estimator_.steps[1])\n",
    "print(grid_cv_k_best.best_estimator_.steps[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 17969,
     "status": "ok",
     "timestamp": 1607725107681,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "1orevPz6X9en",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cddee803391bc050c9ac38fe239f1fdc",
     "grade": false,
     "grade_id": "cell-c70963537664edfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "4b572352-dbe4-4069-8da1-dad8cdff403b"
   },
   "outputs": [],
   "source": [
    "scores = grid_cv_k_best.cv_results_['mean_test_score']\n",
    "heatmap = plt.pcolor(scores.reshape(len(alphas), len(ks)), cmap=\"RdYlGn\")\n",
    "plt.colorbar(heatmap)\n",
    "plt.xticks(np.arange(len(ks)) + 0.5, ks)\n",
    "plt.xlabel(\"number of features\")\n",
    "plt.yticks(np.arange(len(alphas)) + 0.5, alphas)\n",
    "plt.ylabel(\"alpha\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Optimal alpha  = %.4f\" % grid_cv_k_best.best_estimator_.steps[2][1].alpha)\n",
    "print(\"Optimal number of features  = %.4f\" % grid_cv_k_best.best_estimator_.steps[1][1].k)\n",
    "print(\"Optimal R2 score = %.4f\" % grid_cv_k_best.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13609,
     "status": "ok",
     "timestamp": 1607725107682,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "M-Z-UCU3uERD",
    "outputId": "a3139a76-42fd-4c72-f240-def0be8f5dc6"
   },
   "outputs": [],
   "source": [
    "# fit the best model and evaluate performance on test set\n",
    "best_model = grid_cv_k_best.best_estimator_\n",
    "R2_tr, R2_te = evaluate_model(best_model, X_tr, y_tr, X_te, y_te)\n",
    "print(\"Train R2 = %.2f\" % R2_tr)\n",
    "print(\"Test R2 = %.2f\" % R2_te)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
